"""P2S10 TD3 v5 with 60x60 front and orientation from ac3.ipynb

Automatically generated by Colaboratory.


# Twin-Delayed DDPG

On a custom car env
state:
1. 40x40 cutout: 25 embeddings || car is at mid ( grid embeddings)
2. 25 cnn embeddings `+` [distance, orientation, -orientation, self.angle, -self.angle]
NOTE: Embeddings are actually 25 rectangles

Action space: angle and speed with range[-20,20]
    NOTE: predicted action speed is interpolated b/w [3,6]


TODO: add Dabba delivery system in place ?? DONE
"""

# import libraries
import pygame
import gym_dabbewala
from gym import wrappers
import gym
from PIL import Image as PILImage
import math
from collections import deque
from torch.autograd import Variable
import torchvision.transforms as T
import torch.nn.functional as F
import torch.nn as nn
import torch
import matplotlib.pyplot as plt
import numpy as np
import random
import time
import os
import sys

import ai

"""## We make a function that evaluates the policy by calculating its average reward over 10 episodes"""

def evaluate_policy(policy, eval_episodes=10):
    avg_reward = 0.
    for _ in range(eval_episodes):
        obs = env.reset()
        # print(f'pickup{env.x1, env.y1}; drop{env.x2,env.y2}')
        done = False
        while not done:
            action = policy.select_action(obs['surround'], obs['orientation'])
            obs, reward, done, _ = env.step(action)
            env.render()

            avg_reward += reward
    avg_reward /= eval_episodes
    print("---------------------------------------")
    print("Average Reward over the Evaluation Step: %f" % (avg_reward))
    print("---------------------------------------")
    return avg_reward

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    """## We set the parameters"""

    env_name = "DabbeWala-v0"
    # seed = 0 # Random seed number
    start_timesteps = 2e4  # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network
    eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)
    max_timesteps = 5e5  # Total number of iterations/timesteps
    save_models = True  # Boolean checker whether or not to save the pre-trained model
    expl_noise = 0.15  # Exploration noise - STD value of exploration Gaussian noise
    batch_size = 100  # Size of the batch
    discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward
    tau = 0.005  # Target network update rate
    policy_noise = 0.25 # STD of Gaussian noise added to the actions for the exploration purposes
    noise_clip = 0.5# Maximum value of the Gaussian noise added to the actions (policy)
    policy_freq = 2# Number of iterations to wait before the policy network (Actor model) is updated

    """## We create a file name for the two saved models: the Actor and Critic models"""

    file_name = "%s_%s" % ("TD3", env_name)

    """## We create a folder inside which will be saved the trained models"""
    if save_models and not os.path.exists("./pytorch_models"):
        os.makedirs("./pytorch_models")

    """## We create the PyBullet environment"""
    env = gym.make(env_name)
 
    # torch.manual_seed(seed)
    # np.random.seed(seed)
    state_dim = env.observation_space["surround"].shape[2]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])
    min_action = float(env.action_space.low[0])


    """ ## We create the policy network (the Actor model)"""
    policy = ai.TD3(state_dim, action_dim, max_action)

    """## We create the Experience Replay memory"""
    replay_buffer = ai.ReplayBuffer()

    """## We define a list where all the evaluation results over 10 episodes are stored"""
    evaluations = [evaluate_policy(policy)]

    max_episode_steps = env._max_episode_steps

    """## We initialize the variables"""

    total_timesteps = 0
    timesteps_since_eval = 0
    episode_num = 0
    done = True
    t0 = time.time()

    """## Training"""

    max_timesteps = 500000
    # We start the main loop over 500,000 timesteps
    while total_timesteps < max_timesteps:
    
        # If the episode is done
        if done:
            # If we are not at the very beginning, we start the training process of the model
            if total_timesteps != 0:
                # if total_timesteps%100 == 0:
                print("Timesteps: {} Ep_Number: {} Reward: {:.2f} Cocaine: {} Mild_Cocaine {} Sadness: {} Death: {}".format(total_timesteps, episode_num, episode_reward, cocaine, mild_cocaine, sadness, death))
                policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)

            # We evaluate the episode and we save the policy
            if timesteps_since_eval >= eval_freq:
                timesteps_since_eval %= eval_freq
                evaluations.append(evaluate_policy(policy))
                policy.save(file_name, directory=model_path)
                np.save("./results/%s" % (file_name), evaluations)
            
            # When the training step is done, we reset the state of the environment
            obs = env.reset()
            
            # Set the Done to False
            done = False
            
            # Set rewards and episode timesteps to zero
            episode_reward = 0
            episode_timesteps = 0
            episode_num += 1

            #pos and neg reward counter
            cocaine = 0
            sadness = 0
            mild_cocaine = 0
            death = 0
        
    # Before 10000 timesteps, we play random actions
    if total_timesteps < start_timesteps:
        action = env.action_space.sample()
    else: # After 10000 timesteps, we switch to the model
        # action = policy.select_action(np.array(obs))
        action = policy.select_action(obs['surround'], obs['orientation'])
        
        # If the explore_noise parameter is not 0, we add noise to the action and we clip it
        if expl_noise != 0:
            action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)
    
    # The agent performs the action in the environment, then reaches the next state and receives the reward
    new_obs, reward, done, _ = env.step(action)
    
    # We check if the episode is done
    done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)
    
    # We increase the total reward
    episode_reward += reward
    
    # see pos and neg reward counts
    if reward > 0.00:
        cocaine += 1  
    elif reward == -0.01:
        mild_cocaine += 1
    elif reward < -0.6:
        death += 1 
    else:
        sadness += 1 
    
    # We store the new transition into the Experience Replay memory (ReplayBuffer)
    replay_buffer.add((obs['surround'], obs['orientation'], new_obs['surround'], new_obs['orientation'], action, reward, done_bool))

    # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy
    obs = new_obs
    episode_timesteps += 1
    total_timesteps += 1
    timesteps_since_eval += 1

    # We add the last policy evaluation to our list of evaluations and we save our model
    evaluations.append(evaluate_policy(policy))
    if save_models:
        policy.save("%s" % (file_name), directory="./pytorch_models")
    env.close()
